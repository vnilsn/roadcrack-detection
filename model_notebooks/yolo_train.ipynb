{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata.jsonl to ../datasets/Norway/train/images/\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from datasets import load_dataset\n",
    "path_labels = \"../datasets/Norway/train/annotations/xmls/\"\n",
    "path_imgs = \"../datasets/Norway/train/images/\"\n",
    "\n",
    "create_metadata(path_imgs, path_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/2915 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2a2616ea7ee4b778338d471149b36c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration images-d7fc7bd1426f213c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/images to /Users/kohmann/.cache/huggingface/datasets/imagefolder/images-d7fc7bd1426f213c/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading data files #1: 100%|██████████| 183/183 [00:00<00:00, 64549.46obj/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #8: 100%|██████████| 182/182 [00:00<00:00, 61363.61obj/s][A\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #6: 100%|██████████| 182/182 [00:00<00:00, 59106.72obj/s][A\u001B[A\n",
      "Downloading data files #0: 100%|██████████| 183/183 [00:00<00:00, 66374.75obj/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #12:   0%|          | 0/182 [00:00<?, ?obj/s]\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "Downloading data files #2:   0%|          | 0/183 [00:00<?, ?obj/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #11:   0%|          | 0/182 [00:00<?, ?obj/s]\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #9:   0%|          | 0/182 [00:00<?, ?obj/s]\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #7:   0%|          | 0/182 [00:00<?, ?obj/s]\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #4:   0%|          | 0/182 [00:00<?, ?obj/s]\u001B[A\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #12: 100%|██████████| 182/182 [00:00<00:00, 27979.45obj/s]A\n",
      "Downloading data files #9: 100%|██████████| 182/182 [00:00<00:00, 37454.66obj/s]\n",
      "Downloading data files #11: 100%|██████████| 182/182 [00:00<00:00, 29890.10obj/s]\n",
      "Downloading data files #4: 100%|██████████| 182/182 [00:00<00:00, 43343.36obj/s]\n",
      "Downloading data files #2: 100%|██████████| 183/183 [00:00<00:00, 26403.77obj/s]\n",
      "Downloading data files #7: 100%|██████████| 182/182 [00:00<00:00, 37807.11obj/s]\n",
      "Downloading data files #5: 100%|██████████| 182/182 [00:00<00:00, 41111.77obj/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #13:   0%|          | 0/182 [00:00<?, ?obj/s]\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Downloading data files #3:   0%|          | 0/182 [00:00<?, ?obj/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #13: 100%|██████████| 182/182 [00:00<00:00, 60450.06obj/s][A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\n",
      "Downloading data files #3: 100%|██████████| 182/182 [00:00<00:00, 60864.56obj/s]\n",
      "Downloading data files #10: 100%|██████████| 182/182 [00:00<00:00, 60282.98obj/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #14: 100%|██████████| 182/182 [00:00<00:00, 71913.64obj/s][A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #15: 100%|██████████| 182/182 [00:00<00:00, 75082.46obj/s][A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "582465d20e5447288576ae6fe49bbca6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47e6ca38b6c94e749bf0088fa7550cd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83fd9ca4c43d41b5993b37c82c39997e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /Users/kohmann/.cache/huggingface/datasets/imagefolder/images-d7fc7bd1426f213c/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(path_imgs, split=\"train[:50]\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3650x2044>,\n 'annotations': [{'image_id': 0,\n   'category_id': 0,\n   'bbox': [1138, 1281, 31, 55],\n   'area': 1705,\n   'iscrowd': 0},\n  {'image_id': 0,\n   'category_id': 3,\n   'bbox': [1537, 1131, 92, 116],\n   'area': 10672,\n   'iscrowd': 0},\n  {'image_id': 0,\n   'category_id': 0,\n   'bbox': [1773, 1825, 89, 213],\n   'area': 18957,\n   'iscrowd': 0},\n  {'image_id': 0,\n   'category_id': 0,\n   'bbox': [1589, 1296, 35, 47],\n   'area': 1645,\n   'iscrowd': 0},\n  {'image_id': 0,\n   'category_id': 0,\n   'bbox': [1507, 1216, 20, 38],\n   'area': 760,\n   'iscrowd': 0}]}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "ds = dataset.train_test_split(test_size=0.2)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from transformers import YolosFeatureExtractor\n",
    "\n",
    "label2id = {'D00': 0, 'D10': 1, 'D40': 2, 'D20': 3, } # 'pothole': 4\n",
    "id2label = {\"0\":'D00', \"1\":'D10', \"2\":'D40', \"3\":'D20'} # \"4\":'pothole'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small') # , reduce_labels=True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def train_transforms(example_batch):\n",
    "    #images = [x for x in example_batch[\"image\"]]\n",
    "    #labels = [x for x in example_batch[\"annotations\"]]\n",
    "\n",
    "    inputs = feature_extractor(example_batch, example_batch)\n",
    "    return inputs\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotations\"]]\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4040x2035>,\n 'annotations': [{'image_id': 90,\n   'category_id': 0,\n   'bbox': [675, 1141, 291, 144],\n   'area': 41904,\n   'iscrowd': 0},\n  {'image_id': 90,\n   'category_id': 0,\n   'bbox': [296, 1346, 257, 196],\n   'area': 50372,\n   'iscrowd': 0},\n  {'image_id': 90,\n   'category_id': 0,\n   'bbox': [45, 1855, 145, 124],\n   'area': 17980,\n   'iscrowd': 0},\n  {'image_id': 90,\n   'category_id': 0,\n   'bbox': [1010, 1386, 149, 157],\n   'area': 23393,\n   'iscrowd': 0},\n  {'image_id': 90,\n   'category_id': 0,\n   'bbox': [292, 1498, 303, 271],\n   'area': 82113,\n   'iscrowd': 0},\n  {'image_id': 90,\n   'category_id': 0,\n   'bbox': [43, 1739, 174, 134],\n   'area': 23316,\n   'iscrowd': 0},\n  {'image_id': 90,\n   'category_id': 0,\n   'bbox': [776, 1375, 95, 167],\n   'area': 15865,\n   'iscrowd': 0},\n  {'image_id': 90,\n   'category_id': 1,\n   'bbox': [313, 1258, 395, 108],\n   'area': 42660,\n   'iscrowd': 0}]}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = train_ds[4]\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n                    Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of object\n                    detection, each dictionary should contain the keys 'image_id' and 'annotations', with the latter\n                    being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary\n                    should contain the keys 'file_name', 'image_id' and 'segments_info', with the latter being a list\n                    of annotations in COCO format.\n                    ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mfeature_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/transformers/models/yolos/feature_extraction_yolos.py:522\u001B[0m, in \u001B[0;36mYolosFeatureExtractor.__call__\u001B[0;34m(self, images, annotations, return_segmentation_masks, masks_path, padding, return_tensors, **kwargs)\u001B[0m\n\u001B[1;32m    519\u001B[0m                         valid_annotations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    521\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid_annotations:\n\u001B[0;32m--> 522\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    523\u001B[0m             \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    524\u001B[0m \u001B[38;5;124;03m            Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of object\u001B[39;00m\n\u001B[1;32m    525\u001B[0m \u001B[38;5;124;03m            detection, each dictionary should contain the keys 'image_id' and 'annotations', with the latter\u001B[39;00m\n\u001B[1;32m    526\u001B[0m \u001B[38;5;124;03m            being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary\u001B[39;00m\n\u001B[1;32m    527\u001B[0m \u001B[38;5;124;03m            should contain the keys 'file_name', 'image_id' and 'segments_info', with the latter being a list\u001B[39;00m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;124;03m            of annotations in COCO format.\u001B[39;00m\n\u001B[1;32m    529\u001B[0m \u001B[38;5;124;03m            \"\"\"\u001B[39;00m\n\u001B[1;32m    530\u001B[0m         )\n\u001B[1;32m    532\u001B[0m \u001B[38;5;66;03m# Check that masks_path has a valid type\u001B[39;00m\n\u001B[1;32m    533\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m masks_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mValueError\u001B[0m: \n                    Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of object\n                    detection, each dictionary should contain the keys 'image_id' and 'annotations', with the latter\n                    being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary\n                    should contain the keys 'file_name', 'image_id' and 'segments_info', with the latter being a list\n                    of annotations in COCO format.\n                    "
     ]
    }
   ],
   "source": [
    "\n",
    "feature_extractor(batch[\"image\"], batch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/kohmann/.cache/huggingface/hub/models--hustvl--yolos-small/snapshots/5f960fd774250e41a01086ccbbf5e44d9d603c14/config.json\n",
      "Model config YolosConfig {\n",
      "  \"architectures\": [\n",
      "    \"YolosForObjectDetection\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auxiliary_loss\": false,\n",
      "  \"bbox_cost\": 5,\n",
      "  \"bbox_loss_coefficient\": 5,\n",
      "  \"class_cost\": 1,\n",
      "  \"eos_coefficient\": 0.1,\n",
      "  \"giou_cost\": 2,\n",
      "  \"giou_loss_coefficient\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"D00\",\n",
      "    \"1\": \"D10\",\n",
      "    \"2\": \"D40\",\n",
      "    \"3\": \"D20\"\n",
      "  },\n",
      "  \"image_size\": [\n",
      "    512,\n",
      "    864\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"D00\": 0,\n",
      "    \"D10\": 1,\n",
      "    \"D20\": 3,\n",
      "    \"D40\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"yolos\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_detection_tokens\": 100,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_mid_position_embeddings\": true\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/kohmann/.cache/huggingface/hub/models--hustvl--yolos-small/snapshots/5f960fd774250e41a01086ccbbf5e44d9d603c14/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing YolosForObjectDetection.\n",
      "\n",
      "Some weights of YolosForObjectDetection were not initialized from the model checkpoint at hustvl/yolos-small and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.layers.2.weight: found shape torch.Size([92, 384]) in the checkpoint and torch.Size([5, 384]) in the model instantiated\n",
      "- class_labels_classifier.layers.2.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import YolosForObjectDetection\n",
    "\n",
    "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-small',\n",
    "                                                id2label=id2label,\n",
    "                                                label2id=label2id,\n",
    "                                                ignore_mismatched_sizes=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/kohmann/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 40\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14\n",
      "  Number of trainable parameters = 30651273\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Images must of type `PIL.Image.Image`, `np.ndarray` or `torch.Tensor` (single example), `List[PIL.Image.Image]`, `List[np.ndarray]` or `List[torch.Tensor]` (batch of examples).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [26], line 23\u001B[0m\n\u001B[1;32m      3\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m      4\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./results\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m     per_device_train_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m     remove_unused_columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     15\u001B[0m )\n\u001B[1;32m     16\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     17\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     18\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m     19\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mtrain_ds,\n\u001B[1;32m     20\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39mtest_ds,\n\u001B[1;32m     21\u001B[0m )\n\u001B[0;32m---> 23\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/transformers/trainer.py:1501\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[1;32m   1498\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[1;32m   1499\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[1;32m   1500\u001B[0m )\n\u001B[0;32m-> 1501\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1503\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1504\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1505\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1506\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/transformers/trainer.py:1723\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1720\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_rng_state(resume_from_checkpoint)\n\u001B[1;32m   1722\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1723\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(epoch_iterator):\n\u001B[1;32m   1724\u001B[0m \n\u001B[1;32m   1725\u001B[0m     \u001B[38;5;66;03m# Skip past any already trained steps if resuming training\u001B[39;00m\n\u001B[1;32m   1726\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m steps_trained_in_current_epoch \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1727\u001B[0m         steps_trained_in_current_epoch \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:2343\u001B[0m, in \u001B[0;36mDataset.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   2341\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):  \u001B[38;5;66;03m# noqa: F811\u001B[39;00m\n\u001B[1;32m   2342\u001B[0m     \u001B[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001B[39;00m\n\u001B[0;32m-> 2343\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2344\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2345\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:2328\u001B[0m, in \u001B[0;36mDataset._getitem\u001B[0;34m(self, key, decoded, **kwargs)\u001B[0m\n\u001B[1;32m   2326\u001B[0m formatter \u001B[38;5;241m=\u001B[39m get_formatter(format_type, features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures, decoded\u001B[38;5;241m=\u001B[39mdecoded, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mformat_kwargs)\n\u001B[1;32m   2327\u001B[0m pa_subtable \u001B[38;5;241m=\u001B[39m query_table(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data, key, indices\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indices \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m-> 2328\u001B[0m formatted_output \u001B[38;5;241m=\u001B[39m \u001B[43mformat_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2329\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpa_subtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformatter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mformatter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformat_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mformat_columns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_all_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_all_columns\u001B[49m\n\u001B[1;32m   2330\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/datasets/formatting/formatting.py:509\u001B[0m, in \u001B[0;36mformat_table\u001B[0;34m(table, key, formatter, format_columns, output_all_columns)\u001B[0m\n\u001B[1;32m    507\u001B[0m python_formatter \u001B[38;5;241m=\u001B[39m PythonFormatter(features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m format_columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mformatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m query_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    511\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m format_columns:\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/datasets/formatting/formatting.py:282\u001B[0m, in \u001B[0;36mFormatter.__call__\u001B[0;34m(self, pa_table, query_type)\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pa_table: pa\u001B[38;5;241m.\u001B[39mTable, query_type: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m query_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrow\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 282\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_row\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m query_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    284\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat_column(pa_table)\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/datasets/formatting/formatting.py:364\u001B[0m, in \u001B[0;36mCustomFormatter.format_row\u001B[0;34m(self, pa_table)\u001B[0m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformat_row\u001B[39m(\u001B[38;5;28mself\u001B[39m, pa_table: pa\u001B[38;5;241m.\u001B[39mTable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m:\n\u001B[0;32m--> 364\u001B[0m     formatted_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    366\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _unnest(formatted_batch)\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/datasets/formatting/formatting.py:395\u001B[0m, in \u001B[0;36mCustomFormatter.format_batch\u001B[0;34m(self, pa_table)\u001B[0m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoded:\n\u001B[1;32m    394\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_features_decoder\u001B[38;5;241m.\u001B[39mdecode_batch(batch)\n\u001B[0;32m--> 395\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [23], line 5\u001B[0m, in \u001B[0;36mtrain_transforms\u001B[0;34m(example_batch)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_transforms\u001B[39m(example_batch):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;66;03m#images = [x for x in example_batch[\"image\"]]\u001B[39;00m\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m#labels = [x for x in example_batch[\"annotations\"]]\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[43mfeature_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inputs\n",
      "File \u001B[0;32m~/Documents/Studie/2022 Høst/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/transformers/models/yolos/feature_extraction_yolos.py:482\u001B[0m, in \u001B[0;36mYolosFeatureExtractor.__call__\u001B[0;34m(self, images, annotations, return_segmentation_masks, masks_path, padding, return_tensors, **kwargs)\u001B[0m\n\u001B[1;32m    479\u001B[0m         valid_images \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    481\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid_images:\n\u001B[0;32m--> 482\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    483\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImages must of type `PIL.Image.Image`, `np.ndarray` or `torch.Tensor` (single example), \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    484\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`List[PIL.Image.Image]`, `List[np.ndarray]` or `List[torch.Tensor]` (batch of examples).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m     )\n\u001B[1;32m    487\u001B[0m is_batched \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbool\u001B[39m(\n\u001B[1;32m    488\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(images, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m))\n\u001B[1;32m    489\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(images[\u001B[38;5;241m0\u001B[39m], (Image\u001B[38;5;241m.\u001B[39mImage, np\u001B[38;5;241m.\u001B[39mndarray)) \u001B[38;5;129;01mor\u001B[39;00m is_torch_tensor(images[\u001B[38;5;241m0\u001B[39m]))\n\u001B[1;32m    490\u001B[0m )\n\u001B[1;32m    492\u001B[0m \u001B[38;5;66;03m# Check that annotations has a valid type\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: Images must of type `PIL.Image.Image`, `np.ndarray` or `torch.Tensor` (single example), `List[PIL.Image.Image]`, `List[np.ndarray]` or `List[torch.Tensor]` (batch of examples)."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1,\n",
    "    fp16=False,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def prepare_coco_detection( image, target):\n",
    "        \"\"\"\n",
    "        Convert the target in COCO format into the format expected by DETR.\n",
    "        \"\"\"\n",
    "        w, h = image.size\n",
    "\n",
    "        image_id = target[\"image_id\"]\n",
    "        image_id = np.asarray([image_id], dtype=np.int64)\n",
    "\n",
    "        # get all COCO annotations for the given image\n",
    "        anno = target[\"annotations\"]\n",
    "\n",
    "        anno = [obj for obj in anno if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0]\n",
    "\n",
    "        boxes = [obj[\"bbox\"] for obj in anno]\n",
    "        # guard against no boxes via resizing\n",
    "        boxes = np.asarray(boxes, dtype=np.float32).reshape(-1, 4)\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "        boxes[:, 0::2] = boxes[:, 0::2].clip(min=0, max=w)\n",
    "        boxes[:, 1::2] = boxes[:, 1::2].clip(min=0, max=h)\n",
    "\n",
    "        classes = [obj[\"category_id\"] for obj in anno]\n",
    "        classes = np.asarray(classes, dtype=np.int64)\n",
    "\n",
    "        keypoints = None\n",
    "        if anno and \"keypoints\" in anno[0]:\n",
    "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
    "            keypoints = np.asarray(keypoints, dtype=np.float32)\n",
    "            num_keypoints = keypoints.shape[0]\n",
    "            if num_keypoints:\n",
    "                keypoints = keypoints.reshape((-1, 3))\n",
    "\n",
    "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
    "        boxes = boxes[keep]\n",
    "        classes = classes[keep]\n",
    "\n",
    "        if keypoints is not None:\n",
    "            keypoints = keypoints[keep]\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"class_labels\"] = classes\n",
    "\n",
    "        target[\"image_id\"] = image_id\n",
    "        if keypoints is not None:\n",
    "            target[\"keypoints\"] = keypoints\n",
    "\n",
    "        # for conversion to coco api\n",
    "        area = np.asarray([obj[\"area\"] for obj in anno], dtype=np.float32)\n",
    "        iscrowd = np.asarray([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno], dtype=np.int64)\n",
    "        target[\"area\"] = area[keep]\n",
    "        target[\"iscrowd\"] = iscrowd[keep]\n",
    "\n",
    "        target[\"orig_size\"] = np.asarray([int(h), int(w)], dtype=np.int64)\n",
    "        target[\"size\"] = np.asarray([int(h), int(w)], dtype=np.int64)\n",
    "\n",
    "        return image, target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'image_id'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [44], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mprepare_coco_detection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mannotations\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [43], line 8\u001B[0m, in \u001B[0;36mprepare_coco_detection\u001B[0;34m(image, target)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03mConvert the target in COCO format into the format expected by DETR.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      6\u001B[0m w, h \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39msize\n\u001B[0;32m----> 8\u001B[0m image_id \u001B[38;5;241m=\u001B[39m \u001B[43mtarget\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimage_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m      9\u001B[0m image_id \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray([image_id], dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# get all COCO annotations for the given image\u001B[39;00m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'image_id'"
     ]
    }
   ],
   "source": [
    "prepare_coco_detection(batch[\"image\"], batch[\"annotations\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
