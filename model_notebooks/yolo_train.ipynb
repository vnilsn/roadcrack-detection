{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "from utils import *\n",
    "from datasets import load_dataset\n",
    "#from accelerate import Accelerator\n",
    "#accelerator = Accelerator()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata.jsonl with 2914 samples to /Users/kohmann/Documents/Studie/2022 HÃ¸st/Visual_intelligence/roadcrack-detection/model_notebooks\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "path_labels = \"../datasets/Norway/train/annotations/xmls/\"\n",
    "path_imgs = \"../datasets/Norway/train/images/\"\n",
    "\n",
    "create_metadata(path_imgs, path_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a9f09cc8e59b7e3a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/kohmann/.cache/huggingface/datasets/json/default-a9f09cc8e59b7e3a/0.0.0...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6ac05ac61cb4466a6d7986f45f2272c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60ef44ac73144015a01f6e40a9f169c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0 tables [00:00, ? tables/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8998a94756243f9bf54a4538311ab6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/kohmann/.cache/huggingface/datasets/json/default-a9f09cc8e59b7e3a/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from datasets import Dataset, Image\n",
    "dataset = Dataset.from_json(\"metadata.jsonl\")\n",
    "dataset = dataset.cast_column(\"image\", Image())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "2914"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /Users/kohmann/.cache/huggingface/hub/models--hustvl--yolos-small/snapshots/5f960fd774250e41a01086ccbbf5e44d9d603c14/preprocessor_config.json\n",
      "Feature extractor YolosFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"YolosFeatureExtractor\",\n",
      "  \"format\": \"coco_detection\",\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"max_size\": 1333,\n",
      "  \"size\": [\n",
      "    800,\n",
      "    800\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import YolosFeatureExtractor\n",
    "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small', size=(800,800)) # , reduce_labels=True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def transforms(example_batch):\n",
    "    images = example_batch[\"image\"]\n",
    "    ids_ = example_batch[\"image_id\"]\n",
    "    objects = example_batch[\"annotations\"]\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": object_} for id_, object_ in zip(ids_, objects)\n",
    "    ]\n",
    "    inputs = feature_extractor(images=images, annotations=targets , return_tensors=\"pt\")\n",
    "    return inputs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "ds = dataset.train_test_split(test_size=0.2)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]\n",
    "train_ds.set_transform(transforms)\n",
    "test_ds.set_transform(transforms)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "{'pixel_values': tensor([[[ 0.1254,  0.1254,  0.1426,  ..., -0.6794, -0.6794, -0.7822],\n          [ 0.1426,  0.1426,  0.1426,  ..., -0.6794, -0.6623, -0.7479],\n          [ 0.1597,  0.1597,  0.1426,  ..., -0.6794, -0.6794, -0.7650],\n          ...,\n          [-0.8164, -0.7822, -0.7650,  ..., -0.8335, -1.0733, -1.4329],\n          [-0.7993, -0.7822, -0.7822,  ..., -0.9705, -1.2274, -1.5699],\n          [-0.7479, -0.7993, -0.8849,  ..., -1.0904, -1.3644, -1.6727]],\n \n         [[ 0.5203,  0.5203,  0.5378,  ..., -0.2325, -0.2675, -0.5126],\n          [ 0.5378,  0.5378,  0.5378,  ..., -0.2325, -0.2500, -0.4776],\n          [ 0.5553,  0.5553,  0.5378,  ..., -0.2325, -0.2675, -0.4951],\n          ...,\n          [-0.7577, -0.7227, -0.6877,  ..., -0.7052, -0.9328, -1.2654],\n          [-0.7577, -0.7402, -0.7227,  ..., -0.8627, -1.0728, -1.4055],\n          [-0.7052, -0.7752, -0.8627,  ..., -0.9853, -1.2479, -1.5455]],\n \n         [[ 1.0714,  1.0714,  1.0888,  ...,  0.5659,  0.5834,  0.2348],\n          [ 1.0888,  1.0888,  1.0888,  ...,  0.5659,  0.6008,  0.2696],\n          [ 1.1062,  1.1062,  1.0888,  ...,  0.5659,  0.5834,  0.2348],\n          ...,\n          [-0.6367, -0.6193, -0.6541,  ..., -0.3927, -0.6367, -1.0201],\n          [-0.6367, -0.6367, -0.6541,  ..., -0.5321, -0.7761, -1.1247],\n          [-0.6715, -0.6715, -0.7238,  ..., -0.6890, -0.8981, -1.1944]]]),\n 'labels': {'boxes': tensor([[0.3896, 0.8101, 0.0267, 0.0988]]),\n  'class_labels': tensor([0]),\n  'image_id': tensor([7159]),\n  'area': tensor([1689.8728]),\n  'iscrowd': tensor([0]),\n  'orig_size': tensor([2035, 4040]),\n  'size': tensor([800, 800])}}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/kohmann/.cache/huggingface/hub/models--hustvl--yolos-small/snapshots/5f960fd774250e41a01086ccbbf5e44d9d603c14/config.json\n",
      "Model config YolosConfig {\n",
      "  \"architectures\": [\n",
      "    \"YolosForObjectDetection\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auxiliary_loss\": false,\n",
      "  \"bbox_cost\": 5,\n",
      "  \"bbox_loss_coefficient\": 5,\n",
      "  \"class_cost\": 1,\n",
      "  \"eos_coefficient\": 0.1,\n",
      "  \"giou_cost\": 2,\n",
      "  \"giou_loss_coefficient\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"D00\",\n",
      "    \"1\": \"D10\",\n",
      "    \"2\": \"D40\",\n",
      "    \"3\": \"D20\"\n",
      "  },\n",
      "  \"image_size\": [\n",
      "    512,\n",
      "    864\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"D00\": 0,\n",
      "    \"D10\": 1,\n",
      "    \"D20\": 3,\n",
      "    \"D40\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"yolos\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_detection_tokens\": 100,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_mid_position_embeddings\": true\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/kohmann/.cache/huggingface/hub/models--hustvl--yolos-small/snapshots/5f960fd774250e41a01086ccbbf5e44d9d603c14/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing YolosForObjectDetection.\n",
      "\n",
      "Some weights of YolosForObjectDetection were not initialized from the model checkpoint at hustvl/yolos-small and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.layers.2.weight: found shape torch.Size([92, 384]) in the checkpoint and torch.Size([5, 384]) in the model instantiated\n",
      "- class_labels_classifier.layers.2.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import YolosForObjectDetection\n",
    "label2id = {'D00': 0, 'D10': 1, 'D40': 2, 'D20': 3, } # 'pothole': 4\n",
    "id2label = {\"0\":'D00', \"1\":'D10', \"2\":'D40', \"3\":'D20'} # \"4\":'pothole'\n",
    "\n",
    "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-small',\n",
    "                                                id2label=id2label,\n",
    "                                                label2id=label2id,\n",
    "                                                ignore_mismatched_sizes=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "import torch\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = torch.stack(pixel_values)\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/kohmann/Documents/Studie/2022 HÃ¸st/Visual_intelligence/roadcrack-detection/venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2331\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1166\n",
      "  Number of trainable parameters = 30651273\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1166 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 583\n",
      "  Batch size = 8\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# We need to specify loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1,\n",
    "    fp16=False,\n",
    "    save_steps=100,\n",
    "    eval_steps=1,\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "results = trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference (does not currently work)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline, ZeroShotObjectDetectionPipeline\n",
    "object_detector = pipeline('object-detection',model=model ,feature_extractor=feature_extractor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "from utils import *\n",
    "from PIL import Image\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "#image = dataset[16][\"image\"]\n",
    "results = object_detector(image)\n",
    "plot_results(image, results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "#image = dataset[16][\"image\"]\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = trainer.model(**inputs)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_sizes = torch.tensor(image.size)\n",
    "target_sizes\n",
    "results = feature_extractor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs[\"logits\"].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
